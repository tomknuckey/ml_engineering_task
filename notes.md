Determine the likelihood of claiming for each of the 1200 applications the business receives a day.

This data needs to be ETL'd into the warehouse as a batch hob.

This model needs to incorporate new data

We need to know that the pipeline is

* Reliable
* Tested

It now needs to be productionasised

**Task** - Extract the necessary functionality from the notebook to:

1. Clean the dataset
2. Apply the preprocessing 
3. Train the model
4. Evaluate it's performance
5. Build into a cloud based CI/CD pipeline + Deployment framework (e.g Databricks)


**Answer these questions**

* What are the assumptions you made and why
* What considerations are there to ensure the business can leverage this
* What teams within the business would you need to talk to and why
* What is in and out of scope

**Submission**

* Via Git
* Send Link
* Should be on main branch 

**Expectations**

* Task is measured based off process + understanding not on performance / live endpoint
* We should show what is important and why
* Aren't expecting an actual deployment but demonstrate the service locally / describe the deployment steps
* This doesn't need to be finished - focus on structure, where we can do a skeleton - utilising real code, pseudo code and comments / markdown 

